import rlgym
import time
from stable_baselines.common.policies import MlpPolicy
from stable_baselines.common.vec_env import SubprocVecEnv
from stable_baselines import PPO2

MAX_TIMESTEPS = 30000

env = rlgym.make("Default")
model = PPO2(MlpPolicy, env, verbose=1)
model.learn(total_timesteps=MAX_TIMESTEPS)
model.save("agent_no1")

del model
model = PPO2.load('agent_no1')

for i in range(3):
    obs = env.reset()
    done = False
    steps = 0
    ep_reward = 0
    t0 = time.time()
    while True:
        actions, _states = model.predict(obs)  # agent.act(obs) | Your agent should go here
        obs, reward, done, state = env.step(actions)
        ep_reward += reward[0]
        obs = new_obs
        steps += 1
        if done:
            break

    length = time.time() - t0
    print("Step time: {:1.5f} | Episode time: {:.2f} | Episode Reward: {:.2f}".format(length / steps, length, ep_reward))
print('Finished rendering')